{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4-qIq-Rt179",
        "outputId": "b0aec2d9-ec10-4ae7-99b8-d89884e08e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.18.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.18.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18.0) (4.66.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers==4.18.0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18.0) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18.0) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, xxhash, sacremoses, requests, pyarrow, dill, multiprocess, transformers, datasets\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 sacremoses-0.1.1 tokenizers-0.12.1 transformers-4.18.0 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers==4.18.0 sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ5SzANft-Lx"
      },
      "outputs": [],
      "source": [
        "from datasets import *\n",
        "import transformers\n",
        "from tokenizers import *\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbrpX9l_eRoN",
        "outputId": "317b1426-7bd1-438e-dfb8-c2cb9990acf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "99af829a1e8c40419486d849ffa57e9c",
            "2d61d91c099a4a8db607c380a33c04e0",
            "063dc1ce8f5e43d59258024d3908b4fa",
            "e2c864622a744729afb8bb01de1ae14c",
            "0f43adc94dc5457aa60a1b4797ffe654",
            "8f1595f69c214f97923dfc27af7afd6b",
            "719bd2fd90414d848ddebfc4d3f59df3",
            "7d5ec7108a2b4f45a2987e0682e10a6d",
            "750443c721dc4356b0b4afc68ced21b2",
            "7b8d6e46ab4e422dbe44a74fd0c52101",
            "a00e07f60a8a41c3b5042855bdd06602"
          ]
        },
        "id": "klnb5RwbOVV8",
        "outputId": "73ffe36e-9549-4a33-d762-4a2cc00c066c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99af829a1e8c40419486d849ffa57e9c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#local dataset\n",
        "dataset = load_dataset(\"text\",data_files=\"/content/drive/MyDrive/BERT MODEL/DATASET/uncased_july15.txt\",split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPFy5_zzuBra",
        "outputId": "64935289-28ed-4d11-e8d5-3aa547c9c940"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['text'],\n",
              "     num_rows: 253539\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['text'],\n",
              "     num_rows: 28172\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# split the dataset into training (90%) and testing (10%)\n",
        "d = dataset.train_test_split(test_size=0.1)\n",
        "d[\"train\"], d[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8soLvAgXprN",
        "outputId": "75212c33-86e5-476e-afbf-785cc8e12bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"inihayag ng pamunuan ng light rail manila corp lrmc, na namamahala lrt1, na kailangan nilang magbawas ng 100 tauhan dahil na rin paghina ng bunga ng pagbaba ng bilang ng sumasakay tren sanhi ng covid19 pandemic', 'kaniadtong martes, gipahibalo lider light rail manila corp lrmc nagdumala lrt1, kinahanglan magtanggal og usa ka gatos trabahante usab paghinay pagubos kadaghanon nagasakay trn pandemiya covid19\"\n",
            "==================================================\n",
            "\"na nasa lupain ng canaan, anak ni ruben, at anak ni gad, at kalahating lipi ni manases ay nagtayo roon ng dambana tabi ng jordan, isang malaking dambana na matatanaw, 'ug paghiadto dapit haduol jordan maoy yuta canaan, anak ni ruben, anak ni gad, katungangabanay ni manases, nanagtukod usa ka halaran haduol jordan, usa ka dakung halaran pagatanawon\"\n",
            "==================================================\n",
            "\"ug ang tanang nagapuyo sa yuta magasimba kaniya, tanan sila kinsang mga ngalan, sa wala pa ang pagkatukod sa kalibutan, wala mahisulat sa basahon sa kinabuhi nga iya sa gipatay nga cordero.\"\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "for t in d[\"train\"][\"text\"][:3]:\n",
        "  print(t)\n",
        "  print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOA9GizPuFlB"
      },
      "outputs": [],
      "source": [
        "# if you want to train the tokenizer from scratch (especially if you have custom\n",
        "# dataset loaded as datasets object), then run this cell to save it as files\n",
        "# but if you already have your custom data as text files, there is no point using this\n",
        "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
        "  \"\"\"Utility function to save dataset text to disk,\n",
        "  useful for using the texts to train the tokenizer\n",
        "  (as the tokenizer accepts files)\"\"\"\n",
        "  with open(output_filename, \"w\") as f:\n",
        "    for t in dataset[\"text\"]:\n",
        "      print(t, file=f)\n",
        "\n",
        "# save the training set to train.txt\n",
        "dataset_to_text(d[\"train\"], \"train.txt\")\n",
        "# save the testing set to test.txt\n",
        "dataset_to_text(d[\"test\"], \"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOBSHkbFuu5H"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\n",
        "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
        "]\n",
        "# if you want to train the tokenizer on both sets\n",
        "# files = [\"train.txt\", \"test.txt\"]\n",
        "# training the tokenizer on the training set\n",
        "files = [\"train.txt\"]\n",
        "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
        "vocab_size = 30_000\n",
        "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
        "max_length = 256\n",
        "# whether to truncate\n",
        "truncate_longer_samples = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CVoZ3bC_j6K"
      },
      "outputs": [],
      "source": [
        "# initialize the WordPiece tokenizer\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "# train the tokenizer\n",
        "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
        "# enable truncation up to the maximum 512 tokens\n",
        "tokenizer.enable_truncation(max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vix0oz7XzI_w"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model\"\n",
        "# make the directory if not already there\n",
        "if not os.path.isdir(model_path):\n",
        "  os.mkdir(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmeI9Vgx06VB",
        "outputId": "6026c397-222d-4d61-fad5-cf00aec42201"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# save the tokenizer\n",
        "tokenizer.save_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-HZAthp0SNk"
      },
      "outputs": [],
      "source": [
        "# dumping some of the tokenizer config to config file,\n",
        "# including special tokens, whether to lower case and the maximum sequence length\n",
        "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
        "  tokenizer_cfg = {\n",
        "      \"do_lower_case\": True,\n",
        "      \"unk_token\": \"[UNK]\",\n",
        "      \"sep_token\": \"[SEP]\",\n",
        "      \"pad_token\": \"[PAD]\",\n",
        "      \"cls_token\": \"[CLS]\",\n",
        "      \"mask_token\": \"[MASK]\",\n",
        "      \"model_max_length\": max_length,\n",
        "      \"max_len\": max_length,\n",
        "  }\n",
        "  json.dump(tokenizer_cfg, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5JK9kxWD7Ea",
        "outputId": "818ca55f-685e-483b-a493-d8616d9ad721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from transformers) (0.1.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers) (1.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFyBbtw8D9MR"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkJ_tU4B0jNf"
      },
      "outputs": [],
      "source": [
        "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "alivajaLPeLw",
        "outputId": "9a3332ca-6b51-4562-e84d-0c79be556203"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer</b><br/>def __call__(text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]], text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]=None, add_special_tokens: bool=True, padding: Union[bool, str, PaddingStrategy]=False, truncation: Union[bool, str, TruncationStrategy]=False, max_length: Optional[int]=None, stride: int=0, is_split_into_words: bool=False, pad_to_multiple_of: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_token_type_ids: Optional[bool]=None, return_attention_mask: Optional[bool]=None, return_overflowing_tokens: bool=False, return_special_tokens_mask: bool=False, return_offsets_mapping: bool=False, return_length: bool=False, verbose: bool=True, **kwargs) -&gt; BatchEncoding</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/tokenization_distilbert.py</a>Construct a DistilBERT tokenizer.\n",
              "\n",
              "[`DistilBertTokenizer`] is identical to [`BertTokenizer`] and runs end-to-end tokenization: punctuation splitting\n",
              "and wordpiece.\n",
              "\n",
              "Refer to superclass [`BertTokenizer`] for usage examples and documentation concerning parameters.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 56);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "type(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNJZ6CHFPw1S"
      },
      "source": [
        "Testing the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_1JI0USPjIW"
      },
      "outputs": [],
      "source": [
        "example = \"Ipaathag kon paano ginbag-o sang Industrial Revolution ang kalibutan.\"\n",
        "encoding = tokenizer(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egjtoKRRPrgY",
        "outputId": "5f72d896-45ab-4bb8-9955-2d8e709a8ea0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [2, 15616, 393, 6520, 482, 15493, 19, 59, 675, 19592, 8441, 177, 1141, 20, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j13BsEoyEv5K",
        "outputId": "6c0c767c-40ce-4e2a-ed03-d97c9006ee6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/saved_model/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/saved_model/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/saved_model/vocab.txt',\n",
              " '/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/saved_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/saved_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "b3c466c707354d3387f61b9259a624ef",
            "39a0a49d4e584020b065f6a6120cb81c",
            "80d3467d91de4aa88f1ae7da901ba44e",
            "88fcbc23e87b4d31a042827b28d47fec",
            "52d682277bc0470ca7508960a0d44ed4",
            "ca1a327accb44a7f95e3a6e301ab147c",
            "3705b3026fbe4122862913bdaa5a812d",
            "1fa25eec5ed64f59b9d12eb0d7443679",
            "f4a7e17e26bd4f3f9e1fc5fa30739e78",
            "5659af8f11724250bed51c21049d35e0",
            "d88fe7503d264902ba3609281ce87ad2",
            "eb4077e7437d4d5da826880ca6278947",
            "e9478078ad6349cc9b195b81755b9bc1",
            "d04fb1e4a06d46cea7ede201240f8893",
            "025797b314f9447695e282d598b9f1e9",
            "6dec99603f154d7c8924678c8e839135",
            "f97b303b81354283b85f88e949ddcfdc",
            "cc6a8896102d4b8ab134ecd8d19bbe19",
            "78f6f824325243dfa3296a1cc5e74d7d",
            "8fde23fea4194eae91b4fb4adc9a8eb1",
            "4badef0a758a4b05a853c2ebcfc36c45",
            "033bc9c9f0654eff9a16bb19ad90bedc"
          ]
        },
        "id": "sYw3cjdQ0pHT",
        "outputId": "77e8d393-6772-44a1-c64a-b7e45c006fd3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/253539 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3c466c707354d3387f61b9259a624ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/28172 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb4077e7437d4d5da826880ca6278947"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "def encode_with_truncation(examples):\n",
        "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
        "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
        "                   max_length=max_length, return_special_tokens_mask=True)\n",
        "\n",
        "def encode_without_truncation(examples):\n",
        "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
        "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
        "\n",
        "# the encode function will depend on the truncate_longer_samples variable\n",
        "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
        "\n",
        "# tokenizing the train dataset\n",
        "train_dataset = d[\"train\"].map(encode, batched=True)\n",
        "# tokenizing the testing dataset\n",
        "test_dataset = d[\"test\"].map(encode, batched=True)\n",
        "\n",
        "if truncate_longer_samples:\n",
        "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
        "  train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "else:\n",
        "  # remove other columns, and remain them as Python lists\n",
        "  test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
        "  train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVSOwKnFUuo_",
        "outputId": "3ed412cf-477c-4e1e-e715-bcf76ba69484"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253539"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is03dfYDUx5A",
        "outputId": "603d2be1-b726-4d21-9411-ae196d4f8c16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([    2,     8, 13963,   210, 10656,   210,  8755, 12950,  1458,  5312,\n",
              "         17422, 10340,    18,   192, 18135, 23328,   136,    18,   192,  5343,\n",
              "          2021,   760,   512,   210,  2356, 12462,  1036,   192,  1471,  5961,\n",
              "           115,   210,  2043,   210, 21057,   210,   993,   210, 26617,  5632,\n",
              "          9589,   210,  1177,  5405,    13,    18,    13,   744,  3436,    18,\n",
              "          6064,  3777,  8755, 12950,  1458,  5312, 17422, 10340,  7818, 23328,\n",
              "           136,    18,   614, 28745,  2372,   262,   204,   170,  9278,  5356,\n",
              "           364, 26592,  8395,   778,   118, 22062, 18701, 22317,   454,   116,\n",
              "         11617,  1177,     8,     3,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pe5ZkpvVBl1"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
        "# max_seq_length.\n",
        "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= max_length:\n",
        "        total_length = (total_length // max_length) * max_length\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "if not truncate_longer_samples:\n",
        "  train_dataset = train_dataset.map(group_texts, batched=True,\n",
        "                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
        "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
        "                                  desc=f\"Grouping texts in chunks of {max_length}\")\n",
        "  # convert them from lists to torch tensors\n",
        "  train_dataset.set_format(\"torch\")\n",
        "  test_dataset.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ0oYZbk-SSh",
        "outputId": "5a608253-43f6-4e91-e234-2c35db4aa43e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(253539, 28172)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(train_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y_SEFTgX6xq"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForMaskedLM, DistilBertConfig,BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANiy-Qmnpwlt",
        "outputId": "443a8f99-cdae-4e6c-cd2b-aeb2d20e92c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.9.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.9.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "rcMsITnbpyoz",
        "outputId": "9d9965b3-e7fd-49a3-cd67-d5f47ca0fbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT=CBERT_ONE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBcRlF19qAK_",
        "outputId": "bcf493d9-cff9-4e37-abfc-b725c43e582e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_PROJECT=CBERT_ONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mslndt81024t"
      },
      "outputs": [],
      "source": [
        "# initialize the model with the config\n",
        "model_config = DistilBertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
        "model = DistilBertForMaskedLM(config=model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmFCTByJ1OI3"
      },
      "outputs": [],
      "source": [
        "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
        "# Modeling (MLM) task\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKJdnkAd1uYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e38d9d8-27a2-4d23-ce57-e409d734926e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "using `logging_steps` to initialize `eval_steps` to 500\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    report_to=\"wandb\",\n",
        "    output_dir=model_path,          # output directory to where save model checkpoint\n",
        "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,            # number of training epochs, feel free to tweak\n",
        "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
        "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
        "    per_device_eval_batch_size=64,  # evaluation batch size\n",
        "    logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
        "    save_steps=1000,\n",
        "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
        "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "View training arguments for verification"
      ],
      "metadata": {
        "id": "-yWu1Ic_Rz0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRFSFgCaB3_w",
        "outputId": "1e515376-0cf9-4bdb-ae16-cf0d6f0f99c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=1,\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "bf16=False,\n",
              "bf16_full_eval=False,\n",
              "data_seed=None,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_pin_memory=True,\n",
              "ddp_bucket_cap_mb=None,\n",
              "ddp_find_unused_parameters=None,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "do_eval=True,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_delay=0,\n",
              "eval_steps=500,\n",
              "evaluation_strategy=IntervalStrategy.STEPS,\n",
              "fp16=False,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "gradient_accumulation_steps=8,\n",
              "gradient_checkpointing=False,\n",
              "greater_is_better=None,\n",
              "group_by_length=False,\n",
              "half_precision_backend=auto,\n",
              "hub_model_id=None,\n",
              "hub_strategy=HubStrategy.EVERY_SAVE,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "label_names=None,\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=5e-05,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=False,\n",
              "local_rank=-1,\n",
              "log_level=-1,\n",
              "log_level_replica=-1,\n",
              "log_on_each_node=True,\n",
              "logging_dir=/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/runs/Jul15_01-36-59_7e9e9bee5feb,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=500,\n",
              "logging_strategy=IntervalStrategy.STEPS,\n",
              "lr_scheduler_type=SchedulerType.LINEAR,\n",
              "max_grad_norm=1.0,\n",
              "max_steps=-1,\n",
              "metric_for_best_model=None,\n",
              "mp_parameters=,\n",
              "no_cuda=False,\n",
              "num_train_epochs=3,\n",
              "optim=OptimizerNames.ADAMW_HF,\n",
              "output_dir=/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model,\n",
              "overwrite_output_dir=True,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=64,\n",
              "per_device_train_batch_size=10,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=False,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "remove_unused_columns=True,\n",
              "report_to=['wandb'],\n",
              "resume_from_checkpoint=None,\n",
              "run_name=/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model,\n",
              "save_on_each_node=False,\n",
              "save_steps=1000,\n",
              "save_strategy=IntervalStrategy.STEPS,\n",
              "save_total_limit=None,\n",
              "seed=42,\n",
              "sharded_ddp=[],\n",
              "skip_memory_metrics=True,\n",
              "tf32=None,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_legacy_prediction_loop=False,\n",
              "warmup_ratio=0.0,\n",
              "warmup_steps=0,\n",
              "weight_decay=0.0,\n",
              "xpu_backend=None,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hxvoxqISQQa"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpJbkibaSanI",
        "outputId": "131f0293-c5f5-4765-8437-5f15c830df7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizer(name_or_path='/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model', vocab_size=30000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), mlm=True, mlm_probability=0.2, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMKVmXZN2o7c"
      },
      "outputs": [],
      "source": [
        "# initialize the trainer and pass everything to it\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DBbr_nRB85f",
        "outputId": "8cd2c88c-b4ed-415d-f58e-847f01950358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.trainer.Trainer at 0x7e9ead7a5900>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_LOG_MODEL=\"checkpoint\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGpgWcRMqkfJ",
        "outputId": "129d4eb0-cdc7-49c2-80ba-c34fb01335a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_LOG_MODEL=\"checkpoint\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkeJCXkjRPFE",
        "outputId": "0db6c2c3-9e18-47e9-bf37-55cf95679b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertConfig {\n",
              "  \"activation\": \"gelu\",\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"dim\": 768,\n",
              "  \"dropout\": 0.1,\n",
              "  \"hidden_dim\": 3072,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"max_position_embeddings\": 256,\n",
              "  \"model_type\": \"distilbert\",\n",
              "  \"n_heads\": 12,\n",
              "  \"n_layers\": 6,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"qa_dropout\": 0.1,\n",
              "  \"seq_classif_dropout\": 0.2,\n",
              "  \"sinusoidal_pos_embds\": false,\n",
              "  \"transformers_version\": \"4.18.0\",\n",
              "  \"vocab_size\": 30000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "#have a look at our model config\n",
        "model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlO__bheRpi8",
        "outputId": "463e4c72-d327-46d1-ac0b-881d26a9a137"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForMaskedLM(\n",
              "  (activation): GELUActivation()\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(256, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  (vocab_projector): Linear(in_features=768, out_features=30000, bias=True)\n",
              "  (mlm_loss_fct): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "#have a look at our model\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8DJUEn7SOh8"
      },
      "source": [
        "Data Collator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QOkV6psJQ9"
      },
      "source": [
        "Training the Model based on the parameters set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HYsgN58E2tFD",
        "outputId": "64d74c55-6e7a-471a-f45d-408079bb72f5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 253539\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 10\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 9507\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4362' max='9507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4362/9507 1:45:23 < 2:04:21, 0.69 it/s, Epoch 1.38/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.006000</td>\n",
              "      <td>6.465328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.293700</td>\n",
              "      <td>6.038574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.913100</td>\n",
              "      <td>5.681075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>5.627700</td>\n",
              "      <td>5.411565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>5.385400</td>\n",
              "      <td>5.230437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>5.221500</td>\n",
              "      <td>5.073392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>5.064600</td>\n",
              "      <td>4.947770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.973200</td>\n",
              "      <td>4.847064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-1000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-1000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-1000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-2000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-2000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-2000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-3000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-3000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-3000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-4000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-4000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-4000/pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9507' max='9507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9507/9507 3:53:42, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.006000</td>\n",
              "      <td>6.465328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.293700</td>\n",
              "      <td>6.038574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.913100</td>\n",
              "      <td>5.681075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>5.627700</td>\n",
              "      <td>5.411565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>5.385400</td>\n",
              "      <td>5.230437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>5.221500</td>\n",
              "      <td>5.073392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>5.064600</td>\n",
              "      <td>4.947770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.973200</td>\n",
              "      <td>4.847064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.877800</td>\n",
              "      <td>4.783420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.804700</td>\n",
              "      <td>4.696238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>4.738800</td>\n",
              "      <td>4.640833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.682600</td>\n",
              "      <td>4.591876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>4.642000</td>\n",
              "      <td>4.524110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>4.597800</td>\n",
              "      <td>4.510478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>4.554500</td>\n",
              "      <td>4.492288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>4.526600</td>\n",
              "      <td>4.453009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>4.486200</td>\n",
              "      <td>4.425910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.487100</td>\n",
              "      <td>4.403172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>4.477500</td>\n",
              "      <td>4.402588</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-5000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-5000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-5000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-6000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-6000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-6000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-7000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-7000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-7000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-8000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-8000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-8000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-9000\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-9000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/checkpoint-9000/pytorch_model.bin\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 28172\n",
            "  Batch size = 64\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9507, training_loss=5.071243907470639, metrics={'train_runtime': 14024.1534, 'train_samples_per_second': 54.236, 'train_steps_per_second': 0.678, 'total_flos': 5.041226526076109e+16, 'train_loss': 5.071243907470639, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475,
          "referenced_widgets": [
            "bbb4fb83bb2b42529bf14f8a5fffc6ff",
            "94127c80db91480f83388fb48f26e02b",
            "f55c77167da14e63a9f48a30e4dec2e8",
            "b54d7e471e6249b686c5ff0642d6b19b",
            "b45c83f972a045eb8c19dacdd4345e3f",
            "f25cfa1feed34baea694a19302a18a90",
            "eb1d453aa20f4d5a8df5f64c40ba7054",
            "ff99996217604ad19e898159c4feb6de"
          ]
        },
        "id": "VfNhy-sRsmig",
        "outputId": "12ca5644-9509-44c7-fa22-608dce4a5054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbb4fb83bb2b42529bf14f8a5fffc6ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>██▁▁▁▁██▁▁█▁████▁▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▁████▁▁██▁█▁▁▁▁██▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▁████▁▁██▁█▁▁▁▁██▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.40259</td></tr><tr><td>eval/runtime</td><td>154.5771</td></tr><tr><td>eval/samples_per_second</td><td>182.252</td></tr><tr><td>eval/steps_per_second</td><td>2.853</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>9507</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>4.4775</td></tr><tr><td>train/total_flos</td><td>5.041226526076109e+16</td></tr><tr><td>train/train_loss</td><td>5.07124</td></tr><tr><td>train/train_runtime</td><td>14024.1534</td></tr><tr><td>train/train_samples_per_second</td><td>54.236</td></tr><tr><td>train/train_steps_per_second</td><td>0.678</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model</strong> at: <a href='https://wandb.ai/gctanuser-none/CBERT_ONE/runs/93mieknh' target=\"_blank\">https://wandb.ai/gctanuser-none/CBERT_ONE/runs/93mieknh</a><br/> View project at: <a href='https://wandb.ai/gctanuser-none/CBERT_ONE' target=\"_blank\">https://wandb.ai/gctanuser-none/CBERT_ONE</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240715_013638-93mieknh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9tcBewvrS-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e526c062-159b-463d-b043-6411ed414fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT/pytorch_model.bin\n",
            "Saving model checkpoint to /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT\n",
            "Configuration saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT/config.json\n",
            "Model weights saved in /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model('/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer"
      ],
      "metadata": {
        "id": "PdrMA6M7paHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUZSRAxV2vp-",
        "outputId": "e2440266-90df-4723-fefb-6dd69196ed6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT/config.json\n",
            "Model config DistilBertConfig {\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 256,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "loading weights file /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
            "\n",
            "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
            "Didn't find file /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/special_tokens_map.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/tokenizer_config.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/vocab.txt\n",
            "loading file None\n",
            "loading file None\n",
            "loading file None\n",
            "loading configuration file /content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/model/config.json\n"
          ]
        }
      ],
      "source": [
        "# when you load from pretrained\n",
        "model = DistilBertForMaskedLM.from_pretrained('/content/drive/MyDrive/BERT MODEL/DistilBERT_FINAL/uncased_model/distilBERT')\n",
        "#tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "# or simply use pipeline\n",
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJO-1w15ARHs",
        "outputId": "2204706c-fec7-43f8-bbb0-4b168a2c1efb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.13207867741584778,\n",
              "  'token': 308,\n",
              "  'token_str': 'k i n i',\n",
              "  'sequence': 'kinsa kabalo mo kini.'},\n",
              " {'score': 0.08122830092906952,\n",
              "  'token': 934,\n",
              "  'token_str': 'n i m o',\n",
              "  'sequence': 'kinsa kabalo mo nimo.'},\n",
              " {'score': 0.04169512167572975,\n",
              "  'token': 269,\n",
              "  'token_str': 's i y a',\n",
              "  'sequence': 'kinsa kabalo mo siya.'},\n",
              " {'score': 0.027872053906321526,\n",
              "  'token': 170,\n",
              "  'token_str': 'k a',\n",
              "  'sequence': 'kinsa kabalo mo ka.'},\n",
              " {'score': 0.02477985993027687,\n",
              "  'token': 355,\n",
              "  'token_str': 'a k o',\n",
              "  'sequence': 'kinsa kabalo mo ako.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# perform predictions\n",
        "fill_mask(f'Kinsa kabalo mo {fill_mask.tokenizer.mask_token}.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'gituyo niya mosunod {fill_mask.tokenizer.mask_token}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps88Bm8RprFF",
        "outputId": "328aad4b-f11b-4049-d60f-c6077c63de7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.49487438797950745,\n",
              "  'token': 176,\n",
              "  'token_str': 'n g a',\n",
              "  'sequence': 'gituyo niya mosunod nga.'},\n",
              " {'score': 0.024267029017210007,\n",
              "  'token': 457,\n",
              "  'token_str': 's e n t e n c e',\n",
              "  'sequence': 'gituyo niya mosunod sentence.'},\n",
              " {'score': 0.014190644025802612,\n",
              "  'token': 308,\n",
              "  'token_str': 'k i n i',\n",
              "  'sequence': 'gituyo niya mosunod kini.'},\n",
              " {'score': 0.013600281439721584,\n",
              "  'token': 269,\n",
              "  'token_str': 's i y a',\n",
              "  'sequence': 'gituyo niya mosunod siya.'},\n",
              " {'score': 0.008054196834564209,\n",
              "  'token': 446,\n",
              "  'token_str': 'p u l o n g',\n",
              "  'sequence': 'gituyo niya mosunod pulong.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'isulod nako ni sa imong {fill_mask.tokenizer.mask_token}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB24TH8pp7Sl",
        "outputId": "2c5f9baf-271c-45a6-eba7-0761526bdc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.03057894855737686,\n",
              "  'token': 446,\n",
              "  'token_str': 'p u l o n g',\n",
              "  'sequence': 'isulod nako ni sa imong pulong.'},\n",
              " {'score': 0.023940756916999817,\n",
              "  'token': 892,\n",
              "  'token_str': 'k a u g a l i n g o n',\n",
              "  'sequence': 'isulod nako ni sa imong kaugalingon.'},\n",
              " {'score': 0.02178204618394375,\n",
              "  'token': 947,\n",
              "  'token_str': 'k i n a b u h i',\n",
              "  'sequence': 'isulod nako ni sa imong kinabuhi.'},\n",
              " {'score': 0.01725897379219532,\n",
              "  'token': 713,\n",
              "  'token_str': 'n g a l a n',\n",
              "  'sequence': 'isulod nako ni sa imong ngalan.'},\n",
              " {'score': 0.011429117992520332,\n",
              "  'token': 628,\n",
              "  'token_str': 'a m a h a n',\n",
              "  'sequence': 'isulod nako ni sa imong amahan.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = fill_mask.tokenizer.mask_token"
      ],
      "metadata": {
        "id": "FN_Z7xu2qIMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'isulod nako ni sa {mask}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxk_b7GiqK_X",
        "outputId": "03c10b6a-7f28-4a83-e62b-4657b7868370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.013821197673678398,\n",
              "  'token': 468,\n",
              "  'token_str': 'b a l a y',\n",
              "  'sequence': 'isulod nako ni sa balay'},\n",
              " {'score': 0.012323757633566856,\n",
              "  'token': 770,\n",
              "  'token_str': 'b a t a',\n",
              "  'sequence': 'isulod nako ni sa bata'},\n",
              " {'score': 0.012149686925113201,\n",
              "  'token': 947,\n",
              "  'token_str': 'k i n a b u h i',\n",
              "  'sequence': 'isulod nako ni sa kinabuhi'},\n",
              " {'score': 0.009886234998703003,\n",
              "  'token': 1141,\n",
              "  'token_str': 'k a l i b u t a n',\n",
              "  'sequence': 'isulod nako ni sa kalibutan'},\n",
              " {'score': 0.009017960168421268,\n",
              "  'token': 1924,\n",
              "  'token_str': 'l i b r o',\n",
              "  'sequence': 'isulod nako ni sa libro'}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'gikinahanglan ang politika ug {mask}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfPgO2mlqVTR",
        "outputId": "d4893f0a-bc96-48b6-fb37-44e463c75f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.027570301666855812,\n",
              "  'token': 457,\n",
              "  'token_str': 's e n t e n c e',\n",
              "  'sequence': 'gikinahanglan ang politika ug sentence.'},\n",
              " {'score': 0.0068985531106591225,\n",
              "  'token': 1284,\n",
              "  'token_str': 'n u m e r o',\n",
              "  'sequence': 'gikinahanglan ang politika ug numero.'},\n",
              " {'score': 0.006486860569566488,\n",
              "  'token': 1873,\n",
              "  'token_str': 'd a t o s',\n",
              "  'sequence': 'gikinahanglan ang politika ug datos.'},\n",
              " {'score': 0.006241977214813232,\n",
              "  'token': 27,\n",
              "  'token_str': '5',\n",
              "  'sequence': 'gikinahanglan ang politika ug 5.'},\n",
              " {'score': 0.0055777146480977535,\n",
              "  'token': 25,\n",
              "  'token_str': '3',\n",
              "  'sequence': 'gikinahanglan ang politika ug 3.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'Kini adunay {mask} ug mahabog nga paril')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A39NInYnqw-Y",
        "outputId": "503c617d-25d0-48f6-a211-f663096147e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.006957870442420244,\n",
              "  'token': 204,\n",
              "  'token_str': 'u s a',\n",
              "  'sequence': 'kini adunay usa ug mahabog nga paril'},\n",
              " {'score': 0.006138226483017206,\n",
              "  'token': 334,\n",
              "  'token_str': 't a w o',\n",
              "  'sequence': 'kini adunay tawo ug mahabog nga paril'},\n",
              " {'score': 0.0053618173114955425,\n",
              "  'token': 893,\n",
              "  'token_str': 'p a a g i',\n",
              "  'sequence': 'kini adunay paagi ug mahabog nga paril'},\n",
              " {'score': 0.004489441402256489,\n",
              "  'token': 1141,\n",
              "  'token_str': 'k a l i b u t a n',\n",
              "  'sequence': 'kini adunay kalibutan ug mahabog nga paril'},\n",
              " {'score': 0.004450521431863308,\n",
              "  'token': 615,\n",
              "  'token_str': 't u i g',\n",
              "  'sequence': 'kini adunay tuig ug mahabog nga paril'}]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'mao kini atong {mask}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQRHVgI2rAmh",
        "outputId": "5a697e66-740d-477d-f128-5538ef136871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.02187158167362213,\n",
              "  'token': 947,\n",
              "  'token_str': 'k i n a b u h i',\n",
              "  'sequence': 'mao kini atong kinabuhi.'},\n",
              " {'score': 0.01590564101934433,\n",
              "  'token': 334,\n",
              "  'token_str': 't a w o',\n",
              "  'sequence': 'mao kini atong tawo.'},\n",
              " {'score': 0.015655873343348503,\n",
              "  'token': 713,\n",
              "  'token_str': 'n g a l a n',\n",
              "  'sequence': 'mao kini atong ngalan.'},\n",
              " {'score': 0.012047935277223587,\n",
              "  'token': 332,\n",
              "  'token_str': 'd i o s',\n",
              "  'sequence': 'mao kini atong dios.'},\n",
              " {'score': 0.011902854777872562,\n",
              "  'token': 547,\n",
              "  'token_str': 'b u t a n g',\n",
              "  'sequence': 'mao kini atong butang.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'gibuhat nato ni para sa atong {mask}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d-Qoq6r4ELG",
        "outputId": "4189c795-971b-49ab-b43b-c1f3c3de67fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.021120714023709297,\n",
              "  'token': 468,\n",
              "  'token_str': 'b a l a y',\n",
              "  'sequence': 'gibuhat nato ni para sa atong balay'},\n",
              " {'score': 0.01722663827240467,\n",
              "  'token': 892,\n",
              "  'token_str': 'k a u g a l i n g o n',\n",
              "  'sequence': 'gibuhat nato ni para sa atong kaugalingon'},\n",
              " {'score': 0.015681160613894463,\n",
              "  'token': 628,\n",
              "  'token_str': 'a m a h a n',\n",
              "  'sequence': 'gibuhat nato ni para sa atong amahan'},\n",
              " {'score': 0.01428880263119936,\n",
              "  'token': 947,\n",
              "  'token_str': 'k i n a b u h i',\n",
              "  'sequence': 'gibuhat nato ni para sa atong kinabuhi'},\n",
              " {'score': 0.012533009983599186,\n",
              "  'token': 770,\n",
              "  'token_str': 'b a t a',\n",
              "  'sequence': 'gibuhat nato ni para sa atong bata'}]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_mask(f'para ni sa atong {mask}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aqjUL_B4PGF",
        "outputId": "b44982d5-d988-4bed-b02c-8c86908f6202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.019216202199459076,\n",
              "  'token': 947,\n",
              "  'token_str': 'k i n a b u h i',\n",
              "  'sequence': 'para ni sa atong kinabuhi.'},\n",
              " {'score': 0.016643516719341278,\n",
              "  'token': 334,\n",
              "  'token_str': 't a w o',\n",
              "  'sequence': 'para ni sa atong tawo.'},\n",
              " {'score': 0.016280390322208405,\n",
              "  'token': 892,\n",
              "  'token_str': 'k a u g a l i n g o n',\n",
              "  'sequence': 'para ni sa atong kaugalingon.'},\n",
              " {'score': 0.01200205460190773,\n",
              "  'token': 547,\n",
              "  'token_str': 'b u t a n g',\n",
              "  'sequence': 'para ni sa atong butang.'},\n",
              " {'score': 0.011071482673287392,\n",
              "  'token': 713,\n",
              "  'token_str': 'n g a l a n',\n",
              "  'sequence': 'para ni sa atong ngalan.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99af829a1e8c40419486d849ffa57e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d61d91c099a4a8db607c380a33c04e0",
              "IPY_MODEL_063dc1ce8f5e43d59258024d3908b4fa",
              "IPY_MODEL_e2c864622a744729afb8bb01de1ae14c"
            ],
            "layout": "IPY_MODEL_0f43adc94dc5457aa60a1b4797ffe654"
          }
        },
        "2d61d91c099a4a8db607c380a33c04e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f1595f69c214f97923dfc27af7afd6b",
            "placeholder": "​",
            "style": "IPY_MODEL_719bd2fd90414d848ddebfc4d3f59df3",
            "value": "Generating train split: "
          }
        },
        "063dc1ce8f5e43d59258024d3908b4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d5ec7108a2b4f45a2987e0682e10a6d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_750443c721dc4356b0b4afc68ced21b2",
            "value": 1
          }
        },
        "e2c864622a744729afb8bb01de1ae14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b8d6e46ab4e422dbe44a74fd0c52101",
            "placeholder": "​",
            "style": "IPY_MODEL_a00e07f60a8a41c3b5042855bdd06602",
            "value": " 281711/0 [00:01&lt;00:00, 347241.76 examples/s]"
          }
        },
        "0f43adc94dc5457aa60a1b4797ffe654": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f1595f69c214f97923dfc27af7afd6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "719bd2fd90414d848ddebfc4d3f59df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d5ec7108a2b4f45a2987e0682e10a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "750443c721dc4356b0b4afc68ced21b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b8d6e46ab4e422dbe44a74fd0c52101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a00e07f60a8a41c3b5042855bdd06602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3c466c707354d3387f61b9259a624ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39a0a49d4e584020b065f6a6120cb81c",
              "IPY_MODEL_80d3467d91de4aa88f1ae7da901ba44e",
              "IPY_MODEL_88fcbc23e87b4d31a042827b28d47fec"
            ],
            "layout": "IPY_MODEL_52d682277bc0470ca7508960a0d44ed4"
          }
        },
        "39a0a49d4e584020b065f6a6120cb81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca1a327accb44a7f95e3a6e301ab147c",
            "placeholder": "​",
            "style": "IPY_MODEL_3705b3026fbe4122862913bdaa5a812d",
            "value": "Map: 100%"
          }
        },
        "80d3467d91de4aa88f1ae7da901ba44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fa25eec5ed64f59b9d12eb0d7443679",
            "max": 253539,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4a7e17e26bd4f3f9e1fc5fa30739e78",
            "value": 253539
          }
        },
        "88fcbc23e87b4d31a042827b28d47fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5659af8f11724250bed51c21049d35e0",
            "placeholder": "​",
            "style": "IPY_MODEL_d88fe7503d264902ba3609281ce87ad2",
            "value": " 253539/253539 [02:34&lt;00:00, 1668.72 examples/s]"
          }
        },
        "52d682277bc0470ca7508960a0d44ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca1a327accb44a7f95e3a6e301ab147c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3705b3026fbe4122862913bdaa5a812d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fa25eec5ed64f59b9d12eb0d7443679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a7e17e26bd4f3f9e1fc5fa30739e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5659af8f11724250bed51c21049d35e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88fe7503d264902ba3609281ce87ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb4077e7437d4d5da826880ca6278947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9478078ad6349cc9b195b81755b9bc1",
              "IPY_MODEL_d04fb1e4a06d46cea7ede201240f8893",
              "IPY_MODEL_025797b314f9447695e282d598b9f1e9"
            ],
            "layout": "IPY_MODEL_6dec99603f154d7c8924678c8e839135"
          }
        },
        "e9478078ad6349cc9b195b81755b9bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97b303b81354283b85f88e949ddcfdc",
            "placeholder": "​",
            "style": "IPY_MODEL_cc6a8896102d4b8ab134ecd8d19bbe19",
            "value": "Map: 100%"
          }
        },
        "d04fb1e4a06d46cea7ede201240f8893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78f6f824325243dfa3296a1cc5e74d7d",
            "max": 28172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fde23fea4194eae91b4fb4adc9a8eb1",
            "value": 28172
          }
        },
        "025797b314f9447695e282d598b9f1e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4badef0a758a4b05a853c2ebcfc36c45",
            "placeholder": "​",
            "style": "IPY_MODEL_033bc9c9f0654eff9a16bb19ad90bedc",
            "value": " 28172/28172 [00:17&lt;00:00, 1643.98 examples/s]"
          }
        },
        "6dec99603f154d7c8924678c8e839135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97b303b81354283b85f88e949ddcfdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc6a8896102d4b8ab134ecd8d19bbe19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78f6f824325243dfa3296a1cc5e74d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fde23fea4194eae91b4fb4adc9a8eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4badef0a758a4b05a853c2ebcfc36c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033bc9c9f0654eff9a16bb19ad90bedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbb4fb83bb2b42529bf14f8a5fffc6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94127c80db91480f83388fb48f26e02b",
              "IPY_MODEL_f55c77167da14e63a9f48a30e4dec2e8"
            ],
            "layout": "IPY_MODEL_b54d7e471e6249b686c5ff0642d6b19b"
          }
        },
        "94127c80db91480f83388fb48f26e02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b45c83f972a045eb8c19dacdd4345e3f",
            "placeholder": "​",
            "style": "IPY_MODEL_f25cfa1feed34baea694a19302a18a90",
            "value": "0.270 MB of 0.270 MB uploaded\r"
          }
        },
        "f55c77167da14e63a9f48a30e4dec2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb1d453aa20f4d5a8df5f64c40ba7054",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff99996217604ad19e898159c4feb6de",
            "value": 1
          }
        },
        "b54d7e471e6249b686c5ff0642d6b19b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b45c83f972a045eb8c19dacdd4345e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25cfa1feed34baea694a19302a18a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb1d453aa20f4d5a8df5f64c40ba7054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff99996217604ad19e898159c4feb6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}